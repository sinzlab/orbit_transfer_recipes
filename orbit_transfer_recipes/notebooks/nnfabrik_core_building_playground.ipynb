{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo script for Model building in nnfabrik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker Image:\n",
    "sinzlab/pytorch:v3.8-torch1.4.0-cuda10.1-dj0.12.4\n",
    "\n",
    "packages from github: from sinzlab/master, if not otherwise specified\n",
    "- nnfabrik \n",
    "- mlutils \n",
    "- data_port\n",
    "- nndichromacy (master branch of my fork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting anix@sinzlab.chlkmukhxp6i.eu-central-1.rds.amazonaws.com:3306\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nndichromacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-25c389944e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnnfabrik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnndichromacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnndichromacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_nnfabrik\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nndichromacy'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import datajoint as dj\n",
    "dj.config[\"enable_python_native_blobs\"] = True\n",
    "dj.config['schema_name'] = \"nnfabrik_color_mei\"\n",
    "dj.config[\"display.limit\"] = 50\n",
    "schema = dj.schema(\"nnfabrik_color_mei\")\n",
    "\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle \n",
    "from torch import load\n",
    "\n",
    "import nnfabrik\n",
    "from nnfabrik import main, builder\n",
    "from nnfabrik.main import *\n",
    "\n",
    "import nndichromacy\n",
    "from nndichromacy.tables.from_nnfabrik import TrainedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config[\"stores\"] = {'minio': {'access_key': 'AKIAWMAHZGKV5RDF4JPO',\n",
    "  'bucket': 'nnfabrik',\n",
    "  'endpoint': 's3.amazonaws.com',\n",
    "  'location': 'dj-store',\n",
    "  'protocol': 's3',\n",
    "  'secret_key': 'tKIXQs6PYUxs1YxH4BF94nxr5A7DOinFKAjS7HCS'},\n",
    " 'minio_mouse_toliaslab_static': {'access_key': 'AKIAWMAHZGKV5RDF4JPO',\n",
    "  'bucket': 'mousetoliaslabstatic',\n",
    "  'endpoint': 's3.amazonaws.com',\n",
    "  'location': 'dj-store',\n",
    "  'protocol': 's3',\n",
    "  'secret_key': 'tKIXQs6PYUxs1YxH4BF94nxr5A7DOinFKAjS7HCS'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = dict(dataset_hash='5f7e057a90a85e2f6feed6936f759a56')\n",
    "full_key = (TrainedModel&key).fetch(\"KEY\", limit=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the DataLoader and the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static22564-2-12-preproc0 exists already. Not unpacking static22564-2-12-preproc0.zip\n"
     ]
    }
   ],
   "source": [
    "dataloaders = (Dataset&key).get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = (Trainer&full_key).get_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from mlutils import regularizers\n",
    "from mlutils.layers.cores import Core2d, OrderedDict, Iterable\n",
    "from mlutils.layers import Bias2DLayer, Scale2DLayer\n",
    "from mlutils.layers.activations import AdaptiveELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalCore(Core2d, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        hidden_channels,\n",
    "        input_kern,\n",
    "        hidden_kern,\n",
    "        layers=3,\n",
    "        gamma_hidden=0,\n",
    "        gamma_input=0.0,\n",
    "        skip=0,\n",
    "        final_nonlinearity=True,\n",
    "        elu_xshift=0.0,\n",
    "        elu_yshift=0.0,\n",
    "        bias=True,\n",
    "        momentum=0.1,\n",
    "        pad_input=True,\n",
    "        hidden_padding=None,\n",
    "        batch_norm=True,\n",
    "        batch_norm_scale=True,\n",
    "        independent_bn_bias=True,\n",
    "        hidden_dilation=1,\n",
    "        laplace_padding=0,\n",
    "        input_regularizer=\"LaplaceL2\",\n",
    "        stack=None,\n",
    "        use_avg_reg=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_channels:     Integer, number of input channels as in\n",
    "            hidden_channels:    Number of hidden channels (i.e feature maps) in each hidden layer\n",
    "            input_kern:     kernel size of the first layer (i.e. the input layer)\n",
    "            hidden_kern:    kernel size of each hidden layer's kernel\n",
    "            layers:         number of layers\n",
    "            gamma_hidden:   regularizer factor for group sparsity\n",
    "            gamma_input:    regularizer factor for the input weights (default: LaplaceL2, see mlutils.regularizers)\n",
    "            skip:           Adds a skip connection\n",
    "            final_nonlinearity: Boolean, if true, appends an ELU layer after the last BatchNorm (if BN=True)\n",
    "            elu_xshift, elu_yshift: final_nonlinearity(x) = Elu(x - elu_xshift) + elu_yshift\n",
    "            bias:           Adds a bias layer.\n",
    "            momentum:       BN momentum\n",
    "            pad_input:      Boolean, if True, applies zero padding to all convolutions\n",
    "            hidden_padding: int or list of int. Padding for hidden layers. Note that this will apply to all the layers\n",
    "                            except the first (input) layer.\n",
    "            batch_norm:     Boolean, if True appends a BN layer after each convolutional layer\n",
    "            batch_norm_scale: If True, a scaling factor after BN will be learned.\n",
    "            independent_bn_bias:    If False, will allow for scaling the batch norm, so that batchnorm\n",
    "                                    and bias can both be true. Defaults to True.\n",
    "            hidden_dilation:    If set to > 1, will apply dilated convs for all hidden layers\n",
    "            laplace_padding: Padding size for the laplace convolution. If padding = None, it defaults to half of\n",
    "                the kernel size (recommended). Setting Padding to 0 is not recommended and leads to artefacts,\n",
    "                zero is the default however to recreate backwards compatibility.\n",
    "            normalize_laplace_regularizer: Boolean, if set to True, will use the LaplaceL2norm function from\n",
    "                mlutils.regularizers, which returns the regularizer as |laplace(filters)| / |filters|\n",
    "            input_regularizer:  String that must match one of the regularizers in ..regularizers\n",
    "            stack:        Int or iterable. Selects which layers of the core should be stacked for the readout.\n",
    "                            default value will stack all layers on top of each other.\n",
    "                            Implemented as layers_to_stack = layers[stack:]. thus:\n",
    "                                stack = -1 will only select the last layer as the readout layer.\n",
    "                                stack of -2 will read out from the last two layers.\n",
    "                                And stack of 1 will read out from layer 1 (0 indexed) until the last layer.\n",
    "\n",
    "            use_avg_reg:    bool. Whether to use the averaged value of regularizer(s) or the summed.\n",
    "\n",
    "            To enable learning batch_norms bias and scale independently, the arguments bias, batch_norm and batch_norm_scale\n",
    "            work together: By default, all are true. In this case there won't be a bias learned in the convolutional layer, but\n",
    "            batch_norm will learn both its bias and scale. If batch_norm is false, but bias true, a bias will be learned in the\n",
    "            convolutional layer. If batch_norm and bias are true, but batch_norm_scale is false, batch_norm won't have learnable\n",
    "            parameters and a BiasLayer will be added after the batch_norm layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        regularizer_config = (\n",
    "            dict(padding=laplace_padding, kernel=input_kern)\n",
    "            if input_regularizer == \"GaussianLaplaceL2\"\n",
    "            else dict(padding=laplace_padding)\n",
    "        )\n",
    "        self._input_weights_regularizer = regularizers.__dict__[input_regularizer](**regularizer_config)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.gamma_input = gamma_input\n",
    "        self.gamma_hidden = gamma_hidden\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.skip = skip\n",
    "        self.use_avg_reg = use_avg_reg\n",
    "\n",
    "        if use_avg_reg:\n",
    "            warnings.warn(\"The averaged value of regularizer will be used.\", UserWarning)\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        if stack is None:\n",
    "            self.stack = range(self.layers)\n",
    "        else:\n",
    "            self.stack = [*range(self.layers)[stack:]] if isinstance(stack, int) else stack\n",
    "\n",
    "        # --- first layer\n",
    "        layer = OrderedDict()\n",
    "        layer[\"conv\"] = nn.Conv2d(\n",
    "            input_channels,\n",
    "            hidden_channels,\n",
    "            input_kern,\n",
    "            padding=input_kern // 2 if pad_input else 0,\n",
    "            bias=bias and not batch_norm,\n",
    "        )\n",
    "        if batch_norm:\n",
    "            if independent_bn_bias:\n",
    "                layer[\"norm\"] = nn.BatchNorm2d(hidden_channels, momentum=momentum)\n",
    "            else:\n",
    "                layer[\"norm\"] = nn.BatchNorm2d(hidden_channels, momentum=momentum, affine=bias and batch_norm_scale)\n",
    "                if bias:\n",
    "                    if not batch_norm_scale:\n",
    "                        layer[\"bias\"] = Bias2DLayer(hidden_channels)\n",
    "                elif batch_norm_scale:\n",
    "                    layer[\"scale\"] = Scale2DLayer(hidden_channels)\n",
    "\n",
    "        if layers > 1 or final_nonlinearity:\n",
    "            layer[\"nonlin\"] = AdaptiveELU(elu_xshift, elu_yshift)\n",
    "        self.features.add_module(\"layer0\", nn.Sequential(layer))\n",
    "\n",
    "        # --- other layers\n",
    "        if not isinstance(hidden_kern, Iterable):\n",
    "            hidden_kern = [hidden_kern] * (self.layers - 1)\n",
    "\n",
    "        for l in range(1, self.layers):\n",
    "            layer = OrderedDict()\n",
    "\n",
    "            hidden_padding = ((hidden_kern[l - 1] - 1) * hidden_dilation + 1) // 2\n",
    "            layer[\"conv\"] = nn.Conv2d(\n",
    "                hidden_channels if not skip > 1 else min(skip, l) * hidden_channels,\n",
    "                hidden_channels,\n",
    "                hidden_kern[l - 1],\n",
    "                padding=hidden_padding,\n",
    "                bias=bias and not batch_norm,\n",
    "                dilation=hidden_dilation,\n",
    "            )\n",
    "            if batch_norm:\n",
    "                if independent_bn_bias:\n",
    "                    layer[\"norm\"] = nn.BatchNorm2d(hidden_channels, momentum=momentum)\n",
    "                else:\n",
    "                    layer[\"norm\"] = nn.BatchNorm2d(hidden_channels, momentum=momentum, affine=bias and batch_norm_scale)\n",
    "                    if bias:\n",
    "                        if not batch_norm_scale:\n",
    "                            layer[\"bias\"] = Bias2DLayer(hidden_channels)\n",
    "                    elif batch_norm_scale:\n",
    "                        layer[\"scale\"] = Scale2DLayer(hidden_channels)\n",
    "\n",
    "            if final_nonlinearity or l < self.layers - 1:\n",
    "                layer[\"nonlin\"] = AdaptiveELU(elu_xshift, elu_yshift)\n",
    "            self.features.add_module(\"layer{}\".format(l), nn.Sequential(layer))\n",
    "\n",
    "        self.apply(self.init_conv)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        ret = []\n",
    "        for l, feat in enumerate(self.features):\n",
    "            do_skip = l >= 1 and self.skip > 1\n",
    "            input_ = feat(input_ if not do_skip else torch.cat(ret[-min(self.skip, l) :], dim=1))\n",
    "            ret.append(input_)\n",
    "\n",
    "        return torch.cat([ret[ind] for ind in self.stack], dim=1)\n",
    "\n",
    "    def laplace(self):\n",
    "        return self._input_weights_regularizer(self.features[0].conv.weight, avg=self.use_avg_reg)\n",
    "\n",
    "    def group_sparsity(self):\n",
    "        ret = 0\n",
    "        for l in range(1, self.layers):\n",
    "            ret = ret + self.features[l].conv.weight.pow(2).sum(3, keepdim=True).sum(2, keepdim=True).sqrt().mean()\n",
    "        return ret / ((self.layers - 1) if self.layers > 1 else 1)\n",
    "\n",
    "    def regularizer(self):\n",
    "        return self.group_sparsity() * self.gamma_hidden + self.gamma_input * self.laplace()\n",
    "\n",
    "    @property\n",
    "    def outchannels(self):\n",
    "        return len(self.features) * self.hidden_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nnfabrik.utility.nn_helpers import get_module_output, set_random_seed, get_dims_for_loader_dict\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nndichromacy.models.utility import unpack_data_info\n",
    "from nndichromacy.models.readouts import MultipleFullGaussian2d, MultipleGaussian2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_model_fn(dataloaders, seed, hidden_channels=32, input_kern=13,  # core args\n",
    "                                    hidden_kern=3, layers=3, gamma_hidden=0, gamma_input=0.1,\n",
    "                                    skip=0, final_nonlinearity=True, core_bias=False, momentum=0.9,\n",
    "                                    pad_input=False, batch_norm=True, hidden_dilation=1,\n",
    "                                    laplace_padding=None, input_regularizer='LaplaceL2norm',\n",
    "                                    readout_bias=True, init_mu_range=0.2, init_sigma_range=0.5,  # readout args,\n",
    "                                    gamma_readout=0.1, elu_offset=0, stack=None, isotropic=True, data_info=None,\n",
    "                                    ):\n",
    "    \"\"\"\n",
    "    Model class of a stacked2dCore (from mlutils) and a pointpooled (spatial transformer) readout\n",
    "\n",
    "    Args:\n",
    "        dataloaders: a dictionary of dataloaders, one loader per session\n",
    "            in the format {'data_key': dataloader object, .. }\n",
    "        seed: random seed\n",
    "        elu_offset: Offset for the output non-linearity [F.elu(x + self.offset)]\n",
    "\n",
    "        all other args: See Documentation of Stacked2dCore in mlutils.layers.cores and\n",
    "            PointPooled2D in mlutils.layers.readouts\n",
    "\n",
    "    Returns: An initialized model which consists of model.core and model.readout\n",
    "    \"\"\"\n",
    "\n",
    "    if data_info is not None:\n",
    "        n_neurons_dict, in_shapes_dict, input_channels = unpack_data_info(data_info)\n",
    "    else:\n",
    "        if \"train\" in dataloaders.keys():\n",
    "            dataloaders = dataloaders[\"train\"]\n",
    "\n",
    "        # Obtain the named tuple fields from the first entry of the first dataloader in the dictionary\n",
    "        in_name, out_name = next(iter(list(dataloaders.values())[0]))._fields\n",
    "\n",
    "        session_shape_dict = get_dims_for_loader_dict(dataloaders)\n",
    "        n_neurons_dict = {k: v[out_name][1] for k, v in session_shape_dict.items()}\n",
    "        in_shapes_dict = {k: v[in_name] for k, v in session_shape_dict.items()}\n",
    "        input_channels = [v[in_name][1] for v in session_shape_dict.values()]\n",
    "        \n",
    "\n",
    "    core_input_channels = list(input_channels.values())[0] if isinstance(input_channels, dict) else input_channels[0]\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "\n",
    "        def __init__(self, core, readout, elu_offset):\n",
    "            super().__init__()\n",
    "            self.core = core\n",
    "            self.readout = readout\n",
    "            self.offset = elu_offset\n",
    "\n",
    "        def forward(self, x, data_key=None, **kwargs):\n",
    "            x = self.core(x)\n",
    "            x = self.readout(x, data_key=data_key, **kwargs)\n",
    "            return F.elu(x + self.offset) + 1\n",
    "\n",
    "        def regularizer(self, data_key):\n",
    "            return self.core.regularizer() + self.readout.regularizer(data_key=data_key)\n",
    "\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    core = ExperimentalCore(input_channels=core_input_channels,\n",
    "                         hidden_channels=hidden_channels,\n",
    "                         input_kern=input_kern,\n",
    "                         hidden_kern=hidden_kern,\n",
    "                         layers=layers,\n",
    "                         gamma_hidden=gamma_hidden,\n",
    "                         gamma_input=gamma_input,\n",
    "                         skip=skip,\n",
    "                         final_nonlinearity=final_nonlinearity,\n",
    "                         bias=core_bias,\n",
    "                         momentum=momentum,\n",
    "                         pad_input=pad_input,\n",
    "                         batch_norm=batch_norm,\n",
    "                         hidden_dilation=hidden_dilation,\n",
    "                         laplace_padding=laplace_padding,\n",
    "                         input_regularizer=input_regularizer,\n",
    "                         stack=stack)\n",
    "\n",
    "    readout = MultipleGaussian2d(core, in_shape_dict=in_shapes_dict,\n",
    "                                 n_neurons_dict=n_neurons_dict,\n",
    "                                 init_mu_range=init_mu_range,\n",
    "                                 bias=readout_bias,\n",
    "                                 init_sigma_range=init_sigma_range,\n",
    "                                 gamma_readout=gamma_readout)\n",
    "\n",
    "    if readout_bias and data_info is None:\n",
    "        for key, value in dataloaders.items():\n",
    "            _, targets = next(iter(value))\n",
    "            readout[key].bias.data = targets.mean(0)\n",
    "\n",
    "    model = Encoder(core, readout, elu_offset)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4c7519553a30>:90: UserWarning: The averaged value of regularizer will be used.\n",
      "  warnings.warn(\"The averaged value of regularizer will be used.\", UserWarning)\n",
      "/notebooks/ml-utils/mlutils/layers/legacy.py:89: UserWarning: sigma is sampled from uniform distribuiton, instead of a fixed value. Consider setting fixed_sigma to True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = toy_model_fn(dataloaders, seed=1, **model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 35/35 [00:05<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001|00/05] ---> 0.03322412446141243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 35/35 [00:01<00:00, 30.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002|00/05] ---> 0.08768273144960403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 35/35 [00:01<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[003|00/05] ---> 0.11835123598575592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 35/35 [00:01<00:00, 31.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004|00/05] ---> 0.14030104875564575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 35/35 [00:01<00:00, 32.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005|00/05] ---> 0.1557382196187973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 35/35 [00:01<00:00, 32.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[006|00/05] ---> 0.17662179470062256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 35/35 [00:01<00:00, 32.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[007|00/05] ---> 0.18889883160591125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 35/35 [00:01<00:00, 32.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[008|00/05] ---> 0.19927500188350677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 35/35 [00:01<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[009|00/05] ---> 0.20478974282741547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 35/35 [00:01<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010|00/05] ---> 0.21234780550003052\n",
      "Restoring best model! 0.212348 ---> 0.212348\n"
     ]
    }
   ],
   "source": [
    "score, output, model_state = trainer(model, dataloaders, seed=1, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
